## Evaluating web sites automatically: a new methodology and software

**Luís Carlos Silva Eiras** [1]

One of the most challenging issues webmasters face can be summarized in numbers: More than 98 million web sites [2] try to grab the attention of 1,086 billion Internet users [3]. To win this fierce battle, they need a tool that allows them to evaluate and improve the site’s relevance to users. Otherwise, launching and maintaining a site is like putting a message in a bottle and throwing it out to sea. No one can expect much of a response.

The evaluation of an organization’s web site should point out what measures need to be taken so that the site can have as many users as possible, given the organization’s objectives, site content and users’ interests. Currently, there are only subjective evaluation criteria [4]. An analysis of 17 methodologies grouped these criteria into 30 different categories (VILELLA, 2003). None of these methodologies include any type of software or system capable of collecting and examining data automatically even though it is very difficult to analyze a site and its behaviors in its entirety manually. The existing methodologies ignore thousands of pages and analyze only the home page and some of the next pages at the most. The capabilities added to the structure of the HTML language and the easiness to change them – which make the “life cycle of a site close to a zero with constant change and innovation”[5] – also decrease the effectiveness of the manual evaluation.

However, it is possible to create an evaluation system based on a methodology that considers sites like “black boxes” that can be submitted to a series of automated tests already available in the market in order to reveal how well they work and how relevant they are to users. This new methodology is outlined in this article which also discusses its limitations and provides solutions for them by proposing a new web site evaluation software.

**Deconstructing a web site – A new methodology**

One of the objectives of the methodology outlined in this article is to advance solutions for the issues found in the current methodologies. Nine types of data that can be measured automatically were selected for this site evaluation methodology. They can be grouped into four main categories - Structure, Visibility, Usage and User Support – that are explained in detail below.

**Structure**

This category describes the site structure by examining the following items: DNS, IP, site map, site history and metatags. This category helps map the site components in order to give a general idea of the site size, resources used and how it has changed over time.

[1] **DNS and IP.** Every web site has a web address or DNS (Domain Name System) that has a corresponding IP (Internet Protocol) address. To find out the IP address of a particular site, you can use the MS-DOS prompt. On Windows XP, for example, click Start, select All Programs, then Accessories and Command Prompt. Next, type “ping” and the DNS number. The number displayed on the screen is the IP address. However, this information may not be so evident if the site has many DNS and IP numbers.

[2] **Site map.** The site map shows how many pages, images, files and systems are available on the web site and how they are linked. Through the site map, it is possible to determine the site navigation, how difficult it is to find a particular information and how many pages, images, files and resources are available in the site folder, but cannot be accessed because there are no links to them. To analyze the site map, you can use, for example, the Astra SiteManager that can be downloaded at http://my.mercuryinteractive.com/cgi-bin/portal/trynbuy/index.jsp [6].

[3] **Site History.** The analysis of the site history should show how the site has evolved in at least four areas: “look and feel” (for example, if designers have redone the pages initially created by programmers, changes in color, banners, logos, typography and the addition of animation), content (new topics, pages and navigational links), journalism (critical news to keep users interested in coming back), and on-line services (how interactive, real-time services have changed over time). Currently, the history of a web site can be obtained at http://www.archive.org/ [7].

[4] **Metatags.** Even though search engines do not use them as much anymore [8], metatags are still important because they are the only place that contains structured, formal information about the site. To see them, you can go to View on the browser main menu, select Source and verify the information contained in the tags between the codes and . There are programs and sites that automatically analyze the metatags, such as http://www.submitexpress.com/analyzer/ [9].

**Visibility**

It measures how often the site is cited by other sites indexed by search engines. Based on this data, you can define by how much you need to increase the site popularity.

[5] **Site frequency in other sites indexed by search engines.** To measure how many times the site is cited by other sites (number of incoming links), you can use the program LPC, Link Popularity Check, available for download at http://www.axandra.com/arelis/download.htm [10]. Once the software is installed, type the site DNS number and the software will display how many times the site appears on search engines like AltaVista, Alltheweb, HotBot, Google, etc which gives you a measure of how popular the site is on the Internet. To make popularity comparisons, you can also type the addresses of other similar or well-known sites.

![](http://photos1.blogger.com/blogger/6917/4079/320/imagem1.0.jpg)

_Table 1: Example of site indexing on search engines._ Source: Link Popularity Check, 1/10/05.

**Usage**

This category analyzes user behavior on the web site.

[6] **Access Statistics** (Log file [11]). The software WebTrends [12] can be used to gather access statistics. This methodology uses the following data measured in the last 12 months to determine the number of users and their behavior on the web site:
- Number of times the web site was accessed (total number of IP addresses that accessed the site);
- Number of pages viewed (total number of URLs [13] visited by IP);
- Number of hits (total number of actions taken by users such as pages viewed, clicks, downloads, searches, e-mail usage, etc).

![](http://photos1.blogger.com/blogger/6917/4079/320/imagem2.jpg)


_Table 2 – 2004 Access statistics - Detran-MG web site - Department of Motor Vehicles of Minas Gerais State - www.detrannet.mg.gov.br_. Source: WebTrends, Prodemge.

**User Support**

It examines how well the site addresses user needs as far as fast access, user-friendliness and interactivity. These items are analyzed on Route, Accessibility and Online Services.

[7] **Route.** When there is a problem accessing a site – there is no access or only partial access (certain images are not displayed or some functions are not working) – it is important to find out what is causing it by using the user’s own computer. The first test you can do is to use the resources available in the operational system. These resources are similar to those used to find out the IP address: You go to the MS-DOS Prompt (On Windows 98, Start / All Programs / MS-DOS Prompt) and type “tracert" [14] followed by the site DNS. A list of the computers between the user’s computer and the site will be displayed.

By using this method, it is possible to find out if the browser is not working properly, if there are connectivity issues with the service provider, problems with the nodes that should make the connections to the visited site or if the server that hosts the site is down or not working properly.

[8]** Accessibility Test.** Some governments require that web sites maintained by their agencies meet certain minimum standards to make identification, navigation and access easier for visually and hearing impaired users. In Brazil, an accessibility test can be conducted at http://www.acessobrasil.org.br [15] to determine if these standards are being met. After the site is submitted, a report is generated with warnings about possible errors, based on the document ”Accessibility Guidelines for Building and Adapting Brazilian Government Content on the Internet.”

[9] **Online Services.** Currently, there is no test that guarantees with accuracy which online services are available on a web site, even though researchers and governments have placed great importance on user support and issues such as digital inclusion, digital democracy and red tape reduction.

A web site usually consists of HTML pages containing programming code that points to databases that provide users with various services. This code can be written in many different ways and sometimes it does not even include the extension that could identify the database used. This happens because there are a great number of technologies available and no established standards.

It is possible to identify the online services available on the web site only when their extensions – .asp, aspx, .cfm, .jsp, .php, .php3, .php4, .xhtml, .shtml or .xsl - are clearly defined on the page. Currently, to find out what extensions are utilized, you can use, for example, the program Copernic Desktop Search to generate an index of the site folder. The results obtained don’t guarantee that there is an online service for each command displayed since these commands can include access to databases as well as access to other page resources, such as images, forms, links, other visual resources, etc.

**What still needs to be done – Proposing a new software**

The data obtained through tests in the areas [1] to [9] allow the webmaster to discard subjective criteria when evaluating his or her web site. Based on the information unveiled by the tests, the webmaster can take concrete steps to increase the site’s relevance to users by making changes in the “look and feel”, navigation and home page journalism, adding new online services or increasing marketing efforts.

The methodology outlined in this article consists of four software programs, three commands and two web sites developed by six different organizations. It would be better if their functionalities were included in a single software program to make it easier for webmasters to use it. This new program should allow the webmaster to set time intervals for the data gathering process. Additionally, when changes are made to the web site, critical reports should automatically compare the “before” and “after” data creating an online index that measures the relevance and the usage of the site being analyzed.

The software should solve the following limitations in the proposed methodology:

**About the structure**

Besides the main DNS, the program should be able to identify all possible IP and DNS numbers of the web site, specially those related to sub-domains or those that group pages and resources stored on different servers.

In the methodology outlined in this article, the software Astra SiteManager provides a map with the connections among the URLs available on the web site. Since these connections are displayed visually, it is necessary to create an automated analysis of these visuals in order to easily identify which information is buried too far away from the home page and the navigation bar. By doing so, it will be possible to recommend changes to improve usability.

![](http://photos1.blogger.com/blogger/6917/4079/320/imagem3.jpg)

![](http://photos1.blogger.com/blogger/6917/4079/320/imagem4.jpg)


_Figures 1 and 2 – Maps of Agridata (http://www.agridata.mg.gov.br/) and Palácio das Artes (http://www.palaciodasartes.com.br/), on 1/12/05 and 1/20/05 respectively._ SOURCE: Astra SiteManager.

The site history should be analyzed as far as changes in the “look and feel” and services offered (daily journalism, online services, etc.) and how these changes impact the access statistics.

Since the metatags are not used as much anymore, researchers need to find a new way to store structured information about the site for search engines, webmasters, programmers and tester 16] . 

**About Visibility**

Much like the previously mentioned Link Popularity Check software, the new program should measure how many times the site is cited in other sites indexed by search engines. Based on these results, it is possible to define a marketing campaign to promote the web site that includes automatic [17] and traditional initiatives such as adding the site address to letterheads, posters, business cards, e-mail signatures, banners, brochures, newsletters, memos, magazines, promotional videos and advertisement.

**About usage**

The access statistics are currently displayed by day, week and month. However, it is also necessary to measure user behavior for the past 12 months, which will indicate if the number of times the web site is accessed is stable, increasing or decreasing over time. This measurement can also reveal if there are peaks in particular months or if these peaks are caused by specific reasons, such as changes on the web site or external events. This trend analysis should be done automatically or in time intervals set by the webmaster.

The new evaluation software should take into consideration that a site can be accessed from anywhere in the world but the actual number of times the site is accessed is the result of its relevance to users. This relevance can be geographically limited. In other words, Google, Yahoo, Amazon, etc. may interest most users all over the world while the web sites of the DMV, a local school or ice cream parlor may interest only users who live nearby these places. The new software could measure this type of geographical relevance.

The evaluation software should also include recent technological advances that allow the “real world” to have a much more accurate representation in the “virtual world.” For example, in 2005, Google launched a service that offers maps and satellite photos http://www.maps.google.com/. The data on these maps are being cross-referenced with sites where users provide their zip code information to generate maps that indicate their location http://www.hotmaps.frozenbear.com/ [18] . Along with these maps, other statistical information about the population should be analyzed to accurately answer questions like who are the web site users and where they are located. [19]

**About user support**

Currently, webmasters don’t have a system to measure how many e-mails they send and receive via their web sites. So it is important to create this automatic control in the areas of user support and usage.

The accessibility test should be extended to include a usability test. In other words, the “look and feel” and the site map should be analyzed in order to improve the effectiveness of the navigation.

**The creation of a relevance index**

The analysis of the data gathered in each of the nine areas by the software program should generate a series of area indicators that, in turn, should generate an iconographic representation – a seal – that quickly informs the webmaster the relevance of the web site. In other words, this representation is an index that adds up all of the area indicators in a certain point in time. This index is recorded over time so the webmaster can see how it has evolved. There are already systems available in popular search engines that can measure relevance. For now, these systems capture and classify only words from web sites and distribute them in a relevance tree.

The system should be able to measure cause and effect relationships between site changes and access statistics. It is known empirically that new services, new “look and feel”, daily journalism on the home page and promotional campaigns can have a positive impact on access statistics. But it is difficult to evaluate the impact of site map alterations or the increase of incoming links – even empirically. The software program should be able to make an accurate assessment of the impact all of these changes.

The program should also be constantly updated as the number and needs of users increase, new equipment is connected to the Internet, new resources and applications are added to the web sites and more systems and information can be accessed through the web, which will require the creation of new indicators of relevance. 

![](http://photos1.blogger.com/blogger/6917/4079/320/imagem5.0.jpg)

_Figure 3 – Example of web site relevance represented by a seal. (Node Number, Number of communication lines system icon, special icon, system type, system complexity)._ SOURCE: BLANKENSHIP, L. Mapping the Net. Cyberpunk. São Paulo: Devir. 1993. pág. 82.

**A new paradigm to evaluate web sites**

Currently, webmasters do not have a tool that allows them to a) evaluate all aspects of the web site; b) make web site evaluations regularly. Since the current evaluation methodologies were initially developed for print media (book, magazines, newspapers, etc.) and adapted for the digital media, they are not effective to evaluate web sites. Neither are they effective in addressing certain issues, such as a web site with many addresses and ever-changing content or a web site as a point in a sub-network inside a larger network (the Internet).

The new system proposed in this article launches a new way to evaluate web sites since it considers them an informational system in a changing environment. In conclusion, this article proposes a new web site evaluation tool that is as dynamic as the Internet.

**More to explore**

BRAJNIK, G. Automatic web usability: what needs to be done? http://www.tri.sbc.com/hfweb/brajnik/hfweb-brajnik.html. Accessed on: June 26, 2005.

EIRAS, L.C.S. A Methodology to Evaluate Sites Automatically. Belo Horizonte: ECI-UFMG. 2005. 126 p.

SHERMAN, C., PRICE, G. The Invisible Web: Uncovering Information Sources Search Engines Can't see. New York: CyberAge Book. 2001. 439 p.

SWEETLAND, J. H. Reviewing the World Wide Web: theory versus reality. Library Trends, v. 48, n. 4, p. 748-768, Spring 2000.

VILELLA, R. M. Content, Usability and Functionaly: Three dimensions for the evaluation of the state government web portals. Belo Horizonte: ECI-UFMG. 2003. 263 p.

**References**

[1] luiscarloseiras@gmail.com

[2] http://news.netcraft.com/archives/2006/05/index.html Accessed on Out 18, 2006.

[3] http://www.internetworldstats.com/stats.htm. Accessed on Out 18, 2006

[4] Examples of subjective criteria include content, accuracy, authority, objectivity, coverage, etc. which are difficult to measure.

[5] BROWNING, J., REISS, S. Cycle time. In Encyclopedia of the new economy. New York: Wired. March, 1998. p. 113.

[6] Accessed on: June 07, 2005.

[7] Acessed on July 4, 2006.

[8] A critical analysis concerning the use of metatags can be found at
http://searchenginewatch.com/sereport/article.php/2165061, where the author Danny Sullivan demonstrates that search engines are no longer indexing keywords. Accessed on: June 7, 2005.

[9] Accessed on: Oct. 10, 2005.

[10] Accessed on: Oct. 10, 2005.

[11] List of all web site files requested by the user.

[12] Copyright © e.g. Software, Inc.

[13] Uniform Resource Locator: string of characters in a standardized format, which refers to a resource or page on the Internet.

[14] From trace route.

[15] Accessed on: June 8, 2005.

[16] “Professional in charge of detecting, documenting and reporting site or system errors so that the development team can correct them as soon as possible.” http://www.timaster.com.br/ Accessed on: June 28, 2005.

[17] The webmaster can register the site in various search engines by using sites such as http://www.bannermania.com.br/; http://www.addme.com/; http://www.submit-away.com/ and http://www.submit-it.com/. Accessed on: July 12, 2005.

[18] Accessed on: October 10, 2005.

[19] For an introduction about this subject, see RAMOS, C. S. Cartography Visualization and Multimedia Cartography. São Paulo: Editora UNESP, 2003. 178 p.

http://evaluatingwebsitesautomatically.blogspot.com.br/
